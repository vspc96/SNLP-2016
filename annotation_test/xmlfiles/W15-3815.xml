<sec_map><section><chunk>Proceedings of the 2015 Workshop on Biomedical Natural Language Processing (BioNLP 2015), pages 127133, Beijing, China, July 30, 2015. c 2015 Association for Computational Linguistics Measuring the readability of medical research journal abstracts Samuel Severance University of Colorado School of Education Institute of Cognitive Science Boulder, CO USA severans@colorado.edu </chunk></section><section><heading>K. Bretonnel Cohen </heading><chunk>University of Colorado School of Medicine Biomedical Text Mining Group Aurora, CO USA kevin.cohen@gmail.com </chunk></section><section><heading>Abstract </heading><chunk>This study examines whether the readability of medical research journal abstracts changed from 1960 to 2010. Abstracts from medical journals were downloaded from PubMed.org in ten-year batches (1960s, 1970s, etc.). Ab- stracts in each decade underwent processing via a custom Python script to determine their Coleman-Liau Index (CLI) readability score. Analysis using one-way ANOVA found statis- tically significant differences between the mean CLI readability scores of each decade (F(4, 6689135) = 12936.91,p&lt;0.0001). Post- hoc analysis using Tukeys method also found all pairwise comparisons between decades mean CLI readability scores to be statistically significant (p&lt;0.001). Readability scores in- creased from decade to decade beginning with a mean CLI score of 16.0813 in the 1960s and ending with a mean CLI score of 16.8617 in the 2000s. These results indicate a 0.7804 grade level increase in the difficulty of read- ing medical research journal abstracts over time and raises questions about the accessibil- ity of medical research for broader audiences. </chunk></section><section><heading>1 Introduction </heading><chunk>A persistent issue in academic research centers on whether the knowledge published by researchers reaches and is understood by those it could benefit. The medical field takes up this issue in its efforts to translate research into practice, or the idea of translational research (Woolf, 2008). Ideally, practitioners can access and thoroughly compre- hend research to better ensure new treatments and knowledge reaches patients and that patient care revolves around evidence-based practices (Pra- vikoff, Tanner, &amp; Pierce, 2005; Woolf, 2008). Be- yond seeking to leverage new research among medical practitioners, translational research also focuses on supporting patients in becoming more active and involved in their healthcare (Woolf, 2008). With the advent of the information age, pa- tients and patients family members have substan- tial opportunities to research their own medical conditions and their treatment options. Navigating and understanding medical research requires that it proves accessible in terms of its readability. This study is a diachronic analysis of the read- ability of medical research. Specifically, this study seeks to answer whether the readability of medical research journal abstracts has changed from the 1960s to the 2000s. Results from this study may have implications for how researchers could com- municate their findings to patients and how to ad- dress discrepancies between the reading level of medical journals and lay audiences reading abili- ties. </chunk></section><section><heading>2 Relevant Literature </heading></section><section><heading>2.1 Readability of health materials in relation to patients </heading><chunk>Research on the readability of health materials in relation to patients has a strong presence in the literature. Health literacy researchers have found 127 that the vast majority of textual information pa- tients typically encounterfrom informed con- sents to patient education materialssurpass the reading ability of patients (Rudd, Moeykens, &amp; Colton, 1999). Such discrepancies may have pro- found negative influences on patient health out- comes (Paasche-Orlow &amp; Wolf, 2010). Indeed, Baker et al. (1998) found an independent associa- tion between low health literacy and increased hospital admission rates where patients with low literacy became hospitalized twice as often as more literate patients. Additionally, patients with high functional health literacy become more involved in their care, including exploring options beyond those presented by a doctor, whereas patients with low functional health literacy tend to limit deci- sions regarding their care to only those presented to them by doctors (Smith et al. 2009). With impli- cations for personal and community health, a study by Navarra et al. (2014) found that HIV-infected youth with below-grade-level reading skills did not completely adhere to their antiretroviral therapy. Despite growing evidence of the role of health literacy in patient outcomes, the readability of medical information for patients has not improved over time, even for items intended for patients. The lack of readability of informed consents, in particu- lar, has garnered attention in the literature (Mead &amp; Howser, 1992; Rudd et al., 1999). An examina- tion of the readability of informed consents from 1975 to 1982 at the Veterans Administration Med- ical Center found them to have a college reading level and that their reading difficulty may have actually increased over the time period examined (Baker &amp; Taub, 1983). Fifteen years later, a study of surgical consents from across the US also showed similarly difficult reading levels with a given consent requiring an average reading level of 12.6 (Hopper et al., 1998). Beyond informed con- sents, other materials directly aimed at laymen also show readability issues. In an analysis of emergen- cy first-aid instructions, Temnikova (2012a, 2012b) found ten separate categories of readabil- ity/complexity problems. Alamoudi and Hong (2015) found the readability of websites related to microtia and aural atresia lacking in terms of facili- tating comprehension. </chunk></section><section><heading>2.2 Identifying and addressing readability issues </heading><chunk>A significant body of work focuses on addressing readability issues in health contexts. It makes the significance of the corpus-based study reported here clear: it shows that we can address readability problems, but first we must know what the reada- bility issues are. Elhadad (2006), for instance, shows which terms in a medical journal article a lay reader would likely not understood and presents an appli- cation that finds these terms and mines an appro- priate definition from the Web. Achieving usable results with a small corpus, Elhadad and Sutaria (2007) presented a parallel-corpus-driven method for finding technical/lay equivalents of medical terms using measures of association. Leroy et al. (2010) pointed out that perceived and the actual difficulty of text influenced the willingness and ability to learn from health information. The re- searchers manipulated characteristics of health texts and measured perceived and actual difficulty, and found they could improve the perceived diffi- culty of text. Their technique also uncovered some problems with standard readability formulas. Using lexical and grammatical analysis of a medical cor- pus to develop a new metric to estimate text diffi- culty called term familiarity, Leroy et al. (2012) performed an experiment where individuals showed slightly improved understanding for sim- plified documents. An evaluation of a writing as- sistance tool that assists with automated simplification related to term familiarity found that simplified text had strong beneficial effects on both perceived and actual difficulty, with better understanding and more learning after reading simplified text than after reading un-simplified text (Leroy, Kauchak &amp; Mouradi, 2013). In another study, Leroy et al. (2013) examined the effects of lexical simplification and coherence enhancement on readability and showed that they interact in complex ways with both perceived and actual dif- ficulty. Investigating linguistic features, specifical- ly discourse features that correlate with the readability of texts for adults with intellectual dis- abilities, Fung et al. (2009) presented a tool for rating the readability of texts for these readers. Huenefaurth et al. (2009) compared different methods for evaluating text readability software for adults with intellectual disabilities, finding that multiple-choice questions with illustrations proved more useful than yes/no questions or Likert scales for evaluating simplification programs. 128 </chunk></section><section><heading>2.3 Work presented in context of relevant literature </heading><chunk>Specific research utilizing a diachronic, corpus- based approach to examining the readability of medical journals did not turn up in a review of the literature. However, previous studies taking a dia- chronic approach to the readability of corpus data do have precedence. Indeed, the inspiration for this study comes from work by Stajner (2011). Stajner performed a diachronic analysis of the Brown family of corpora to examine changes in the readability of the English language over time. Sim- ilar to this study, Stajner utilized the Coleman-Liau Index as a measure of the readability of the Brown family of corpora. </chunk></section><section><heading>3 Methodology </heading><chunk>This study occurred in three main phases in order to answer the research question: How has the read- ability of medical journal abstracts changed be- tween the 1960s and 2000s? </chunk></section><section><heading>3.1 Obtaining a medical research corpus </heading><chunk>The first phase of this study involved compiling a machine-readable corpus of medical research jour- nal abstracts. PubMed.org contains a large volume of medical research journal abstracts and these provided the basis of a corpus. Abstracts were downloaded in groups by decade (see Table 1). This study focused solely on journal abstracts deal- ing with research on human subjects with the as- sumption that a human-centered research corpus has more meaningful parallels to the potential in- terests of most patients. </chunk></section><section><heading>3.2 Measure the readability of abstracts </heading><chunk>The Coleman-Liau Index measure of readabil- ity (Coleman &amp; Liau, 1975) formula is as follows: i ii ii i14 = 5.89 i i i i 29.5 i i  i i 15.8 In this formula, c is equal to the total number of characters in a given text, w is equal to the total number of words in a given text, and s is equal to the total number of sentences in a given text. The CLI outcome measure is given as a grade-level readability score. For example, a grade of 10.5 would correspond to a text at a reading level of halfway through 10 th grade. Figure 1: Distribution of CLI Scores by decade. </chunk></section><section><heading>3.3 Statistical analyses </heading><chunk>The next phase of the study involved creating a database and running analyses to determine the mean CLI scores for abstracts in each decade and whether the differences between these mean scores were statistically significant. A statistically signifi- cant difference in the mean CLI scores for each decade would indicate changes in the readability of medical journal abstracts over time. In order to avoid type 1 errors, the analysis did not engage in a series of t-tests to compare the mean CLI scores for each decade. Rather a one- way ANOVA was deemed more appropriate after checking that the data met certain assumptions. Specifically, ANOVA requires that the data have an approximately normal distribution. Evidence for normality includes histograms of each decades CLI scores with each distribution closely following a normal curve (see Figure 1 above). A Shapiro- Wilk test for normality could not be done because it has an upper limit of 2,000 to 5000 observations (Razali &amp; Wah, 2011), and the data sets in this pa- per surpass that (see Table 1 above). However, Table 1. Number of abstracts by decade. Decade range Number of abstracts 1960-1969 5324 1970-1979 313053 1980-1989 1049637 1990-1999 2017482 2000-2009 3327954 129 examination of the quantile-quantile plots (Figure 2 below) is consistent with the data being approx- imately normally distributed in each decade with only a small fraction of overall observations dis- playing deviations in the tails of some plots. Figure 2: Quantile-quantile plots, by decade. ANOVA also requires very similar variances for each group. Levenes test for homogeneity of variance, run on subsets created through random sampling of each decade, gave statistically signifi- cant values, which means the null hypothesis that the variances were the same could not be rejected. Another assumption for running ANOVA is that the data are independent. Although it is possible that some research articles may have been repub- lished or had text cited in different decades, such instances likely were rare and not significant given the size of the corpus. With the above assumptions addressed, the one- way ANOVA was carried out. Examination of the output indicated that a statistically significant dif- ference did exist between the mean CLI scores for each decade. A one-way ANOVA, however, is an omnibus test and does not indicate between which groups the statistically significant difference exists, just that a statistically significant difference exists somewhere in the data. To determine between which decades there exists a statistically significant difference in mean CLI scores, a post hoc analysis using Tukeys method was carried out. </chunk></section><section><heading>4 Results </heading><chunk>The mean CLI scores for each decade were calcu- lated (see Table 2.) The 1960s had a mean CLI score of 16.0813 with a 95% confidence interval (CI) of 16.00567 to 16.1569. The 1970s had a mean CLI score of 16.3123 with a 95% CI of 16.3024 to 16.32212. The 1980s had a mean CLI score of 16.3867 with a 95% CI of 16.38137 to 16.39194. The 1990s had a mean CLI score of 16.4302 with a 95% CI of 16.42657 to 16.43385. The 2000s had a mean CLI score of 16.8617 with a 95% CI of 16.85901 to 16.86446. Note that none of the 95% CIs overlap between decades. A one-way ANOVA indicated a statistically significant difference between the mean CLI scores for each decade (F(4, 6689135) = 12936.91, p&lt;0.0001; see table 3). In determining which pairs of mean CLI Scores for each decade had a statisti- cally significant difference, a pairwise comparison of means post hoc analysis using Tukeys method indicated that all possible combinations of CLI Scores for each decade had statistically significant differences (p&lt;0.001). </chunk></section><section><heading>5 Analysis </heading><chunk>Having confirmed the statistical significance of the differences between all pairings of the mean CLI scores for each decade, we can consider the mean CLI scores for each decade statistically distinct Table 3. One-way ANOVA results comparing mean CLI scores by decade. Source SS df MS F-statistic p-value Between groups 353331.527 4 88332.8818 12936.91 &lt;0.0001 Within groups 45673229.4 6689135 6.82797244 Total 46026560.9 6689139 6.88079003 Table 2. Mean CLI scores by decade. Decade Mean CLI Score Number of Abstracts 1960s 16.0813 5324 1970s 16.3123 313053 1980s 16.3867 1049637 1990s 16.4302 2017482 2000s 16.8617 3327954 130 from one another. Given this, we can make higher- level observations based on what patterns the indi- vidual means reveal as part of a group. More im- portantly, we can make assertions that allow us to answer our research question: How has the reada- bility of medical journal abstracts changed between the 1960s and 2000s? According to the results of this study, the aver- age difficulty in readability of medical research journal abstracts increased over time. Specifically, readability scores increased from decade to decade beginning with a mean CLI score of 16.0813 in the 1960s and ending with a mean CLI score of 16.8617 in the 2000s. The mean CLI score, there- fore, increased 0.7804 grade level units within the timespan examined. We should also note the high mean CLI scores for each decade. All scores fell within the level of readability expected for a grade level of 16 or a senior in college. </chunk></section><section><heading>6 Future work </heading><chunk>The work reported here discusses only one reada- bility metric. Fleshing out the data with additional readability metrics would prove useful. Experi- mental assessment of comprehension by lay read- ers would be a useful addition to the metrics; for example, by asking them to read abstracts and an- swer questions. Specific subdomains of the bio- medical literature may have their own readability issues, such as formulae and gene names, and iden- tifying these might have implications for ap- proaches to addressing specific readability issues. </chunk></section><section><heading>7 Conclusion </heading><chunk>This study sought to determine whether the reada- bility of medical research journal abstracts changed between the 1960s and 2000s. The results here in- dicate an increase in difficulty of 0.7804 grade lev- els during this time period. Medical journal abstracts, we can conclude, have become more and more difficult to read. For patients attempting to learn more about medical conditions or their treatment options through the reading primary literature, this task has become more difficult to achieve. Importantly, however, the high overall mean CLI scores for each decade indicate that this task likely has al- ways proven difficult for patients. Medical journal abstracts have had readability scores equivalent to grade levels of 16 since the 1960s, well above the average American who reads between a 7th and 8 th grade level (NCES, 2003) and certainly above the 9th-grade level considered difficult (USDHHS, 2000). This consistent difficulty mirrors other re- search showing a lack of progress in the readability of medical-related text (Rudd et al., 1999). From this studys results and the US Depart- ment of Health and Human Services recommenda- tions for the reading levels of medical information text, the readability gap between published medical research and the average American patients read- ing ability appears equal to 7 grade levels. Bridg- ing this chasm in accessibility will likely require interventions for both the researcher and patient. Shoring up the health literacy of Americans would involve a concerted effort to increase the average reading ability of patients. Purposefully addressing health literacy in K-12 education set- tings and Adult Basic Education settings may prove beneficial (Nielsen-Bohlman et al., 2004; Rudd et al., 1999). Such efforts, however, will likely not bridge the 7 grade level gap entirely. Instead, the medical research community should consider taking stepsfor example, developing reading guides or parallel publications aimed at lay readersto increase the readability of their re- search given patients information needs and to support patient self-advocacy. Despite a desire by patients to access and comprehend research that would increase their in- volvement in their own care, members of the med- ical research and publishing community continue to place a premium on complex writing skills put- ting such research out of the reach of most patients. Lakoff (1992) makes a strong case for academics in general being rewarded for difficult writing, and perhaps even being published for incomprehensi- ble writing. With typical reading levels of almost 17, most scientific writing is now beyond the read- ing level of not only the average patient but also most health professionals who typically have a bachelors degree equivalent to a grade level of 16. </chunk></section><section><heading>Acknowledgments </heading><chunk>We thank the participants in LING 5200, Compu- tational Corpus Linguistics, in Fall 2014 for their input into this project. Noemie Elhadad and Gondy Leroy provided helpful comments on a late draft of the work. 131 </chunk></section><section><heading>References </heading><chunk>Alamoudi, U., and Hong, P. (2015) Readability and quality assessment of websites related to microtia and aural atresia. International Jour- nal of Pediatric Otorhinolaryngology 79(2):151-156. Baker, M. T., &amp; Taub, H. A. (1983). Readability of informed consent forms for research in a Vet- erans Administration medical cen- ter. Jama, 250(19), 2646-2648. Baker, D. W., Parker, R. M., Williams, M. V., &amp; Clark, W. S. (1998). Health literacy and the risk of hospital admission. Journal of general internal medicine,13(12), 791-798. Coleman, M., &amp; Liau, T. L. (1975). A computer readability formula designed for machine scor- ing. Journal of Applied Psychology, 60(2), 283. Elhadad, N. (2006) Comprehending technical texts: predicting and defining unfamiliar terms. American Medical Informatics Association Symposium Proceedings, pp. 239-243. Elhadad, N., and Sutaria, K. Mining a lexicon of technical terms and lay equivalents. BioNLP 2007, pp. 49-56. Feng, L., Elhadad, N., and Huenefauth, M. (2009) Cognitively motivated features for readability assessment. EACL 2009, pp. 229-237. Hartley, J. (2004). Current findings from research on structured abstracts.Journal of the Medical Library Association, 92(3), 368. Hopper, K. D., TenHave, T. R., Tully, D. A., &amp; Hall, T. E. (1998). The readability of currently used surgical/procedure consent forms in the United States.Surgery, 123(5), 496-503. Huenefauth, M., Feng, L, and Elhadad, N. (2009) Comparing evaluation techniques for text readability software for adults with intellectual disabilities. ACM SIGACCESS conference on computers and accessibility, pp. 3-10. Joint Commission. (2007) What did the doctor say? Improving health literacy to protect pa- tient safety. Health Care at the Crossroads se- ries. http://www.jointcommission.org/nr/rdonl yres/d5248b2e-e7e6-4121-887499c7b4888301 /0/improving_health_literacy.pdf Lakoff, R.T. (1992) Talking power: the politics of language. Basic Books. Leroy, G., Endicott, J. E., Kauchak, D., Mouradi, O., &amp; Just, M. (2013). User evaluation of the effects of a text simplification algorithm using term familiarity on perception, understanding, learning, and infor- mation retention. Journal of medical Internet re- search, 15(7). Leroy, G., Endicott, J. E., Mouradi, O., Kauchak, D., &amp; Just, M. L. (2012). Improving perceived and actual text difficulty for health information consumers us- ing semi-automated methods. In AMIA Annual Symposium Proceedings (Vol. 2012, p. 522). Amer- ican Medical Informatics Association. Leroy, G., Helmreich, S., &amp; Cowie, J. R. (2010). The influence of text characteristics on perceived and actual difficulty of health information. Internation- al journal of medical informatics, 79(6), 438-449. Leroy, G., Kauchak, D., &amp; Mouradi, O. (2013). A user- study measuring the effects of lexical simplification and coherence enhancement on perceived and actu- al text difficulty. International journal of medical informatics, 82(8), 717-730. Meade, C. D., &amp; Howser, D. M. (1991, Decem- ber). Consent forms: how to determine and improve their readability. In Oncology nursing forum (Vol. 19, No. 10, pp. 1523-1528). Navarra, A.M., Neu, N., Toussi, S., Nelson, J., &amp; Larson, E.L. (2014) Health literacy and adher- ence to antiretroviral therapy among HIV- infected youth. J. Assoc. Nurses AIDS Care. 25(3):203-213. National Center for Education Statistics (2003). National Assessment of Adult Literacy (NAAL). http://nces.ed.gov/naal. Pravikoff, D. S., Tanner, A. B., &amp; Pierce, S. T. (2005). Readiness of US nurses for evidence- based practice: many dont understand or value research and have had little or no training to help them find evidence on which to base their practice. AJN The American Journal of Nurs- ing, 105(9), 40-51. Razali, N. M., &amp; Wah, Y. B. (2011). Power comparisons of Shapiro-Wilk , Kolmogorov- Smirnov , Lilliefors and Anderson-Darling tests. Journal of Statistical Modeling and Analytics, 2(1), 2133. Roter, D. L., Rudd, R. E., Keogh, J., &amp; Robinson, B. (1986). Worker produced health education material for the construction trades. International Quarterly of Community Health Education, 7(2), 109-121. Rudd, R. E., Moeykens, B. A., &amp; Colton, T. C. (1999). Health and literacy: a review of medi- cal and public health literature. Office of Edu- cational Research and Improvement. 132 Smith, S. K., Dixon, A., Trevena, L., Nutbeam, D., &amp; McCaffery, K. J. (2009). Exploring patient involvement in healthcare decision making across different education and functional health literacy groups. Social science &amp; medi- cine,69(12), 1805-1812. Stajner, S. (2011, September). Towards a Better Exploitation of the Brown 'Family Corpora in Diachronic Studies of British and American English Language Varieties. In RANLP Stu- dent Research Workshop (pp. 17-24). Temnikova, I. (2012a) Improving emergency in- structions. Communicator (pp. 48-53). Temnikova, I. (2012b) Text complexity and text simplification in the crisis management do- main. University of Wolverhampton doctoral thesis. United States Department of Health and Human Services. (2000) Saying it clearly. http://www.talkingquality.gov/docs/section3/3 _4.htm. Weiss, B. D., Coyne, C., Michielutte, R., Davis, T. C., Meade, C. D., Doak, L. G., ... &amp; Furnas, S. (1998). Communicating with patients who have limited literacy skills-Report of the Na- tional Work Group on Literacy and Health.Journal of Family Practice, 46(2), 168- 176. Woolf, S. H. (2008). The meaning of translational research and why it matters. JAMA, 299(2), 211-213. 133 </chunk></section></sec_map>