<sec_map><section><chunk>Proceedings of the Eighth SIGHAN Workshop on Chinese Language Processing (SIGHAN-8), pages 158163, Beijing, China, July 30-31, 2015. c 2015 Association for Computational Linguistics and Asian Federation of Natural Language Processing Topic-Based Chinese Message Polarity Classification System at SIGHAN8-Task2 Chun Liao, Chong Feng, Sen Yang, Heyan Huang School of Computer Science and Technology, Beijing Institute of Technology {cliao, fengchong, syang, hhy63}@bit.edu.cn </chunk></section><section><heading>Abstract </heading><chunk>This paper describes the topic-based Chi- nese message polarity classification sys- tem submitted by LCYS_TEAM at SIGHAN8-Task2. The system mainly in- cludes two parts: 1) a graph-based rank- ing model integrating local and global in- formation is adopted to represent the classification ability of words towards different topics. In construction of graph model, a new weighting approach and a PMI-based random jumping probability selection method is proposed. 2) For sen- timental features, word embedding is employed for acquiring expanded topical words and syntactic dependency is adopted for getting topic-related senti- mental words. Experiment results demonstrate the effectiveness of our sys- tem. </chunk></section><section><heading>1 Introduction </heading><chunk>Sentiment analysis, which is to identify or de- termine the implied emotional orientation, atti- tude and opinion when people express something, is becoming more and more important for net- work monitoring with its application on mi- croblog. In the traditional sentiment analysis, unsupervised methods were adopted in Ku(2005), Shen(2009), Vasileios(2000) and Turney(2002), and the limitation of such approaches based on semantic dictionary mainly is unable to solve the problem of Out-of-Vocabulary words. Super- vised methods were employed with model of machine learning, such as Naive Bayes, Max Entropy, Support Vector Machine in Pang(2002), Dasgupta(2009), and Li(2011). Hashtags, in the form of # topic # , are widely used as topics in Chinese microblogs. For the topic-related work, Wang(2011) and Jakob(2010) made research on hashtag-level sen- timent classification in twitter. In the traditional sentiment analysis, the object people express sentiment on is not taken into consideration. And these methods are mostly topic-ignored and can- not perform the accurate sentiment analysis in many topic-related messages. We summarize such kind of difficult cases into two categories. 1) Microblogs with multiple candidate topics For example, # galaxy s6## P8##mate8# galaxy s6 , P8 , mate8[]. This sentence conveys negative sentiment to- wards topic of galaxy s6, but positive sentiment towards topic of P8 and ma- te8. 2) Microblogs with topic specific sentimental words For example, ##, , and ## ,. The word carrys positive sentiment orientation in the first sentence towards topic and negative sen- timent orientation in the latter towards topic . Considering the importance of topical infor- mation in microblogs, this paper studied topic- based Chinese message polarity classification. Given a message from Chinese Weibo Platform (Such as Sina, Tencent, NetEase etc. ) and a top- ic, classify whether the message is of positive, negative, or neutral sentiment towards the given topic. For messages conveying both a positive and negative sentiment towards the topic, which- ever is the stronger sentiment should be chosen. 158 The rest of this paper is organized as follows. In Section 2, we briefly present the topic-based Chinese message polarity classification system from two aspects of graph-based ranking feature and topic-related sentimental feature. Evaluation results are presented in Section 3. Finally, the last section summarizes this paper and describes our future work. </chunk></section><section><heading>2 System Architecture </heading><chunk>In topic-based Chinese message polarity classifi- cation, our system is mainly composed by two parts: topic-related keyword feature selection and topic-related sentimental feature selection. In detail, topic-related keyword feature is acquired by a novel graph-based ranking algorithm of LT- IGT, and topic-related sentimental feature is ob- tained by topical words expansion based on word embedding and syntactic parsing according to the expanded topical words. The architecture of our system is illustrated in Figure 1. Raw test sentences Preparations Preparations TextRank topical words LTIGT global local Expanded topical words Word Embedding Parser Topic-related sentimental words Common features Support Vector Machine Output Result Topic-related keyword feature Topic-related sentimental feature Figure 1. System architecture </chunk></section><section><heading>2.1 Preparations </heading><chunk>To evaluate the performance of method proposed in this paper for topic-based Chinese microblog polarity classification, we carry out experiments on dataset offered by SIGHAN8-Task 2 called Topic-Based Chinese Message Polarity Classifi- cation. This dataset is obtained from Chinese Weibo Platform, such as Sina, Tencent, NetEase etc. It contains 5*1000 manually annotated mi- croblogs which cover 5 topics, such as S6, , etc. In experiments, we ran- domly select 800 microblogs of each topic for training and 200 for testing, and finally get train- ing set of 4000 microblogs and testing set of 1000 microblogs to perform classification. Considering the non-standard feature of mi- croblog, the corpus is firstly normalized by fol- lowing three rules. Rule 1: Turn over the microblog with // to ensure the forwarding relationship and guarantee the latter sentence is analyzed based on the front sentence. Rule 2: Delete structures like @+username, http://xxx to reduce noises caused by username and website. Rule 3: Replace the consecutive punctuations with the first one to normalize the structure of expression. Through filtration by these rules, this paper conducts experiments on the preprocessed da- taset and accesses them with traditional Preci- sion(P), Recall(R) and F-measure(F) under Mi- cro-average and Macro-average. </chunk></section><section><heading>2.2 Selection of topic-related keyword fea- ture </heading><chunk>Inspired by TF-IDF(Salton et al., 1975,1983), words own higher local importance and lower global importance are more significant for classi- fication. But for topic-based Chinese message polarity classification, it is obviously insufficient to extract keywords based on frequency infor- mation merely. For example, in the sentence of GALAXY S6 , ,, microSD,, iphone, the conventional TF-IDF method tends to extract GALAXY S6iphone as keywords, but in this topic-based task, topic-related words such as GALAXY S6 are ex- pected to be selected as the keywords feature for the topic . To better solve the problem of microblogs with multiple candidate topics intro- duced in section 1, this paper proposes a novel LT-IGT(illustrated in Figure 2) algorithm which integrates topic, position and co-occurrence in- formation, its function is designed as follows. i ii ii ii ii ii i i ii ii ii i = i ii ii ii i i ii i i ii ii ii i = i ii ii ii i i ii i i ii i (i ii i i ii i ) 1 i ii ii ii i i ii ii ii i (i ii i i ii i ) (1) 159 where i ii ii ii i i ii i i ii i (i ii i i ii i ) and i ii ii ii i i ii ii ii i (i ii i i ii i ) represent for rank- ing score of vertex i ii i i ii i under local and global Tex- tRank. Microblog 2 Microblog 1 ...... Loc...l TextR...nk ...... Glob...l TextR...nk Topic 1 Topic 2 Figure 2. Graph Model of LT-IGT The idea of TextRank(Mihalce,2004) derives from PageRank, which is achieved by dividing the text into several units to build graph model and exploiting voting mechanism for ranking. This method can model the relationship between the current word and contextual information, and the contextual related words can be recommend- ed reciprocally. Considering the importance of a word is related to both itself and its relevant words, TextRank overcomes the independence of words in traditional bag-of-words model and characterizes the importance of a word more ac- curately. CST: A novel weighting method of graph- based ranking model For each vertex in the graph, its importance ranking score benefits from adjacent nodes, and on the other hand, its own ranking score can also be transferred to the neighboring vertexes. Ac- cording to the above assumptions, the indicator of vertex importance can be divided into follow- ing three parts: Coverage Importance, Semantic Similarity Importance and Topic-Related Im- portance. For two vertexes i ii i i ii i and i ii i i ii i , the influ- ence of i ii i i ii i to i ii i i ii i can be transferred by the directed edge e =&lt; v i ,v j &gt;. In this paper, we assign i ii i i ii i i ii i as the weight between i ii i i ii i and i ii i i ii i , , , as the proportions of these three indicators. Conse- quently, the weight value between two vertexes can be defined as follows: i ii i i ii i i ii i = i i14i i14i ii i i ii ii ii ii ii i i ii i i ii i , i ii i i ii i + i i12i i12i ii i i i i i i i i i  i ii i i ii i , i ii i i ii i + i i34i i34i ii i i ii ii ii i (i ii i i ii i , i ii i i ii i ) (2) Where ++=1. a) i ii i i ii ii ii ii ii i (i ii i i ii i , i ii i i ii i ) represents for coverage im- portance of i ii i i ii i , it can be calculated by i ii i i ii ii ii iv (i ii i i ii i , i ii i i ii i ) = 1 |Out(i ii i i ii i )| (3) Where |Out(i ii i i ii i )| is the out-degree of vertex i ii i i ii i . This formula expresses the coverage importance of i ii i i ii i can be transmitted to its neighboring ver- texes uniformly. b) i ii i i i i i i i i i  (i ii i i ii i , i ii i i ii i ) is regarded as semantic similari- ty importance from i ii i i ii i to i ii i i ii i . It can be ex- pressed as i ii i i i i i i i i i  (i ii i i ii i , i ii i i ii i ) = PMIi ii i i ii i ,i ii i i ii i PMI(i ii i i ii i ,i ii i i ii i ) i ii i i ii i Outi ii i i ii i (4) PMIi ii i i ii i , i ii i i ii i = log ( i ii i(i ii i i ii i &amp; i ii i i ii i ) i ii i(i ii i i ii i )i ii i(i ii i i ii i ) ) (5) Where PMIi ii i i ii i , i ii i i ii i is the point mutual infor- mation between i ii i i ii i and i ii i i ii i . The larger the PMI value is, the higher the semantic similarity is. i ii i(i ii i i ii i &amp; i ii i i ii i ) is the co-occurrence probability of i ii i i ii i and i ii i i ii i in sentences. p(i ii i i ii i ) and pi ii i i ii i respective- ly represent for the independent occurrence probability of i ii i i ii i and i ii i i ii i . This function suggests that words with higher mutual information can substantially influence each other mutually. c) i ii i i ii ii ii i (i ii i i ii i , i ii i i ii i ) shows the topic-related im- portance value of i ii i i ii i . It can be computed by i ii i i ii ii ii i (i ii i i ii i , i ii i i ii i ) = Pi ii i i ii i i ii i(i ii i i ii i ) i ii i i ii i Outi ii i i ii i (6) Where Pi ii i i ii i is the position importance score of i ii i i ii i which can be designed with differ- ent strategies. Considering the importance of topical words in measuring position im- portance score, this paper assigns words occur- ring in topic or existing dependency with topi- cal words a higher score than others. If we as- sign vertex v occurring in topic or existing dependency with topical words as X, the function is P(v) = , v X 1, i ii ii ii i hi ii ii ii ii ii i (7) Where i i14i i14 &gt;1. We set i i14i i14 =1.5 through investi- gation and evaluation in experiments. Selection of Random Jumping Probability In the traditional graph model of TextRank, each vertex jumps to others randomly with an equal probability, which is shown in the function of i ii i i ii ii ii i (i ii i i ii i )= 1 |i ii i| . But this method will bring about the problem of local optimization for its negligence of topical information. Considering the importance of topical words in charactering the main idea of an article, we assign topic- related words with a higher random jumping probability to get a larger score in ranking of graph model. Consequently, this paper adopts 160 PMI value between current word and topical word as the random jumping probability, and the function is as follows. i ii i i ii ii ii i (i ii i i ii i ) = i ii ii ii ii ii i(i ii i i ii i ,topic) i ii ii ii ii ii i(i ii i i ii i ,i ii ii ii ii ii ii ii i i ii i) |i ii i| i ii i=1 (8) where i ii ii ii ii ii i(i ii i i ii i , topic) denotes the point mutu- al information value between current word i ii i i ii i and topical word topic, |i ii i| is the number of ver- texes in graph model. Moreover, the calculation of co-occurrence probability for PMI is per- formed in unit of sentence in global TextRank, but in unit of window in local TextRank. The size of the window is assigned as 5 through experi- ments. Consequently, in the construction of graph model G = (V, E) , vertexes, directions and weights of the links are three important points which should be considered. In this graph model, we denote the vertexes set as V = {i ii i 1 , i ii i 2 , i ii i 3 ... ... i ii i i ii i } which is combined of nouns and adjectives. Furthermore, the direction of a link between two vertexes is determined by a method of sliding window which adds links from the first word pointing to other words with- in the window. And the size of the sliding win- dow is assigned as 10 through experiments. And the weight of a link is set by method of CST pro- posed in this paper. The basic formula of Tex- tRank is performed for calculating the final rank- ing scores of each vertex. Finally, we can acquire two ranking scores for a vertex under global and local TextRank separately. </chunk></section><section><heading>2.3 Selection of topic-related sentimental feature </heading><chunk>In recent years, the method of word embedding based on neural network shows its outperfor- mance in semantic expression and has attracted widespread attention paid to it(Tomas, 2013). The task of word embedding is to represent each word in corpus with a real vector, and establish- ing a mapping between discrete vocabulary and the feature vectors in real fields. Considering the semantic similarity between two words can be characterized by cosine value of the vectors, we propose a novel approach of topic-related senti- mental word embedding which integrates syntax with semantics in this paper. This method ex- pands topical words with word embedding first, and then performs parsing in center of these topi- cal words to extract topical-related sentiment words based on the dependencies with them. Fi- nally we cluster the topical-related sentiment words using K-means clustering algorithm and select the number of words belonging to a cate- gory in a microblog as the dimension values to finish the feature selection of this part. Expansion of Topical-words For example, S6, . Its dependency analysis result is illustrat- ed in Figure 3 as follows. Figure 3. Example of dependency analysis result As we can see in Figure 3, the sentimental words , do not exist dependencies with topical words , S6, but exist de- pendencies with words , of SBV(,), SBV(,). And these relationships also occupy a necessary place in topic-based sentiment analysis of Chinese mi- croblog. So we should obtain , as the expanded topical words from topical words , S6. There are many approaches to expand the top- ical words such as PMI(Turney, 2003), and Syn- onyms-based method(Wang, 2009). For its better consideration of contextual information, we adopt word embedding to calculate the semantic similarity with topical words to expand the topi- cal words. After getting word vectors, we calcu- late the cosine similarity between topical words and nouns under each topic, and select the high- est N words as the expansion of topical words to fulfill the expansion of topical words. Extraction of topic-related sentimental words As we all know, people usually express emotions towards a specific topic or object, and the emo- tional words often exist dependency relationship with topics or objects in syntactic analysis. Con- sequently, we mainly take following three de- pendency relations into consideration: 1) VOB VOB represents for the relation between verbs and objects. Sentimental words are verbs and topical words are the objects of verbs. For example, . It exits VOB rela- tion between and . 2) SBV VOB represents for the relation between sub- jects and predicates. Sentimental words are predicates and topical words are the subjects 161 of sentimental words. For example, . It exits SBV relation between and . 3) ATT ATT represents for the relation of attributes. Sentimental words are attributes and topical words are the modified center of sentimental words. For example, !. It exits ATT relation between and . Therefore, we design an algorithm of topical- related sentimental words extraction towards de- pendency analysis result of microblogs. The pro- cess of this algorithm is described as below. Algorithm1: Topical-related Sentimental Words Extraction Input: Dependency analysis result(DP), Expand- ed Topical Words(ETW) Output: Topical-related Sentimental Words (TSW) for word in DP: if word in ETW and word.relate in SBV, VOB, ATT: TSW+= word.parent; if word.parent in ETW and word.relate in SBV, VOB, ATT: TSW+= word; return TSW </chunk></section><section><heading>3 Experiments </heading><chunk>In SIGHAN8-Task2, we select emoticons, basic sentiment lexicon, dependency relation of SBV, VOB, ATT as common features(C), LT-IGT Ranking score as topic-related keyword fea- ture(TK) and dependency parsing of topical words with word embedding for expansion as topic-related sentimental feature(TS). Table 1 shows the evaluation results of our system with different groups of features. By attempting different groups of feature for topic-related Chinese microblog sentiment classi- fication, the performance of sentiment classifica- tion is notably improved after adding topic- related keyword feature(TK) and topic-related sentimental feature(TS). This is mainly because these two features explore both the syntactic and semantic information for classification compared with the other features. Consequently, this exper- iment not only demonstrates the effectiveness of LT-IGT algorithm, but also reveals the im- portance of topical word expansion to topic- related Chinese microblog sentiment classifica- tion. Method Precision Recall F-measure C 0.6113 0.5572 0.5830 C+TK 0.6458 0.5982 0.6211 C+TK+TS 0.6863 0.6081 0.6448 Table 1: results of topic-based Chinese message po- larity classification using SVM with different groups of features </chunk></section><section><heading>4 Conclusion </heading><chunk>In this paper we proposed a novel method for topic-based Chinese microblog sentiment classi- fication, and put forward two novel feature gen- eration approaches of LT-IGT and topic-related sentimental word embedding, with other kinds of features together, for addition to SVM classifier to perform the final polarity determination. The experimental results demonstrated the effective- ness of these two proposed features, which re- minds us deep processing on syntax and seman- tics might be helpful for traditional regarded shallow works. To further improve the performance of our system, we will try to extend our work in the fol- lowing aspects: 1) Perform phrase structure anal- ysis on microblog to excavate the relation be- tween topical and sentimental words; 2) Investi- gate the impact on other classifiers other than SVM classifier. </chunk></section><section><heading>Acknowledgments </heading><chunk>We would like to thank SIGHAN8 for offering the dataset. We would also acknowledge the help of HIT-IR-Lab for proving the Chinese depend- ency parser(Che, 2010). </chunk></section><section><heading>References </heading><chunk>Lun-wei Ku and Tung-ho Wu and Li-ying Lee and Hsin-hsi Chen. 2005. Construction of an Evaluation Corpus for Opinion Extraction. In Journal of NTCIR, pages 513--520, Taipei, Taiwan. Yang Shen, Shuchen Li, Ling Zheng, Xiaodong Ren and Xiaolong Cheng. 2009. Emotion mining re- search on microblog. In Proceedings of Computer Science &amp; Education (ICCSE), pages 477-480, LanZhou, China. Vasileios Hatzivassiloglou and Janyce M. Wiebe. 2000. Effects of Adjective Orientation and Grada- bility on Sentence Subjectivity. In Proceedings of the the 18th conference on Computational linguis- tics - Volume 1. Association for Computational Linguistics, pages 299-305, New York. Turney P D. 2002. Thumbs Up or Thumbs Down? Semantic Orientation Applied to Unsupervised Classification of Reviews. In Journal of Proc An- 162 nual Meeting of the Association for Computational Linguistics,pages 417--424. Xiaolong Wang Y and Furu Wei Z. 2011. Topic Sen- timent Analysis in Twitter: A Graph-based Hashtag Sentiment Classification Approach. In Internation- al Conference on Information &amp; Knowledge Man- agement Proceedings(2011). Pang B, Lee L, Vaithyanathan S. 2002. Thumbs up? Sentiment Classification using Machine Learning Techniques. Proceedings of Emnlp, pages: 79-86. Dasgupta, S., &amp; Ng, V. 2009. Mine the Easy, Classify the Hard: A Semi-Supervised Approach to Auto- matic Sentiment Classification. In Meeting of the Association for Computational Linguistics, pages 701-709. Li F, Liu N, Jin H, et al. 2011. Incorporating Review- er and Product Information for Review Rating Pre- diction. In Proceedings of the twenty-second inter- national joint conference on artificial intelligence, pages 1820-1825. Jakob N, Darmstadt T U, Gurevych I. 2010. Extract- ing opinion targets in a single and cross-domain setting with conditional random fields. In Proceed- ings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 427. Salton G, Yu C T. 1975. On the construction of effec- tive vocabularies for information retrieval. In Pro- ceedings of Acm Sigplan Notices, pages 48-60. Salton G, Fox E. 1983. Extended Boolean information retrieval. In Journal of Communications of the Acm, 26(11), pages:1022-1036. Mihalcea R, Tarau P. 2004. TextRank: Bringing Or- der into Texts In Proceedings of Unt Scholarly Works. Tomas Mikolov, Kai Chen, Greg Corrado, Jeffrey Dean. 2013. Efficient estimation of word represen- tations in vector space. Turney P D, Littman M L. 2003. Measuring Praise and Criticism: Inference of Semantic Orientation from Association. In Journal of Acm Transactions on Information Systems , 21(4), pages:315-346. Wang S G, De-Yu L I, Wei Y J, et al. 2009. A Syno- nyms Based Word Sentiment Orientation Discrimi- nating. In Journal of Chinese Information Pro- cessing, pages:68-74. Wanxiang Che, Zhenghua Li, and Ting Liu. 2010. LTP: A Chinese Language Technology Platform. In Proceedings of the Coling 2010: Demonstration Volume, pages 13-16, Beijing, China. 163 </chunk></section></sec_map>