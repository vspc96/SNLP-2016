<sec_map><section><chunk>Proceedings of the SIGDIAL 2015 Conference, pages 139143, Prague, Czech Republic, 2-4 September 2015. c 2015 Association for Computational Linguistics Conversational Knowledge Teaching Agent that Uses a Knowledge Base Kyusong LEE, Paul Hongsuck SEO, Junhwi CHOI, Sangjun KOO, Gary Geunbae LEE Department of Computer Science and Engineering, Pohang University of Science and Technology, South Korea {Kyusonglee, hsseo, chasunee, giantpanda, gblee}@postech.ac.kr </chunk></section><section><heading>Abstract </heading><chunk>When implementing a conversational ed- ucational teaching agent, user-intent un- derstanding and dialog management in a dialog system are not sufficient to give us- ers educational information. In this paper, we propose a conversational educational teaching agent that gives users some edu- cational information or triggers interests on educational contents. The proposed system not only converses with a user but also answer questions that the user asked or asks some educational questions by in- tegrating a dialog system with a knowledge base. We used the Wikipedia corpus to learn the weights between two entities and embedding of properties to calculate similarities for the selection of system questions and answers. </chunk></section><section><heading>1 Introduction </heading><chunk>Dialog is the most natural interaction between a mentor and mentee in the real world. Therefore, dialog-based intelligent tutoring systems (ITSs) have been widely studied to teach science (Jordan et al., 2013; Litman and Silliman, 2004; Graesser et al., 2004; VanLehn et al., 2002; Vanlehn et al., 2005), foreign language (Kyusong et al., 2014;Lee et al., 2010; Lee et al., 2011;Johnson et al., 2007), and programming language (Fossati et al., 2008; Lane and VanLehn, 2015) usually with- out intervention from a human teacher. However, previous dialog-based language learning systems mostly only play the role of a conversational part- ner using chatting like spoken dialog technology, and providing feedback such as grammatical error correction and suggesting better expressions. 1 http://isoft.postech.ac.kr/research/language_learning/db- call/videos/e3-1.mp4 However, in real situations, students usually ask many questions to indulge their curiosity and a tu- tor also asks questions to continue the conversa- tion and maintain students interest during the learning process. In science and programming language learning, mostly pre-designed scenarios and contents are necessary; these are usually handcrafted by human education experts. How- ever, this process is expensive and time-consum- ing. Our group is currently involved in a project called POSTECH Immersive English Study (POMY). The program allows users to exercise their visual, aural and tactile senses to receive a full immersion experience to develop into inde- pendent EFL learners and to increase their memory and concentration abilities to the greatest extent (Kyusong Lee et al., 2014). During field tests, we found that many advanced students asked questions that cannot be answered using only a dialog system 1 . Recently, knowledge base (KB) data such as freebase and DBpedia have be- come publicly available. Using the KB, knowledge base question answering (KB-QA) has been studied (Berant and Liang, 2014); it has ad- vantages of very high precision because it exploits huge databases. Hence, we proposed a dialog- based intelligent tutoring system that uses a KB, as an extension of POMY, POMY Intelligent Tu- toring System (POMY-ITS). The main advantage is that the human cost to manually construct edu- cational contents is eliminated. Moreover, the sys- tem chooses its response after considering infor- mation importance, current discourse, relative weights between two entities, and property simi- larity. The additional functions of the POMY-ITS are that it: 1) Answers users question such as factoid ques- tions, word meaning; 139 2) Generates questions to continue the conver- sation and to interest the user; 3) Uses entities and properties in freebase to generate useful information that might inter- est a user, and presents it in natural language. To implement 1) the QA function, we used Par- asempre (Berant and Liang, 2014) based KB-QA system as our QA system. However, in this paper, we focus only on 2) and 3) which are generating questions or informing by selecting appropriate entity and property in the KB; we do not present the detailed explanation or assess the accuracy of the QA system. </chunk></section><section><heading>2 Intuition of the system </heading><chunk>A user who asks about Bill Gates, may also be in- terested in Microsoft and Paul Allen, which are topics strongly related to Bill Gates. In the KB graph, the Bill Gates entity is connected to many other entities. However, these connections present too much information, such as URLs of related websites, gender of Bill Gates, published books, music, and architecture. However, KB does not contain the entity importance or weighted rela- tionship between entities and properties (Figure 1). This information can be useful to POMY-ITS to enable it to decide what to ask or talk about. When a system and a user are talking about Bill Gates wifes name, the user may also want to know when they got married or who Bill Gates other family members are. Manual construction of the entity relationship or order of scenarios would be very expensive. Our system considers entity and property to decide automatically what to ask or to inform. To deploy the system, we used the Wik- ipedia corpus to learn property similarity, and weight between two entity pairs. </chunk></section><section><heading>3 Method </heading><chunk>The main role of the POMY-ITS is to give infor- mation that a user wants to know. The KB-QA technology will give the answer if the utterance is a wh question, but often, a user does not know what to ask. Thus, the conversation must include initiative dialog. When the dialog between a tutor and a user stalls, the tutor should ask a relevant question to or give useful information related to the current context. </chunk></section><section><heading>3.1 The Role of Dialog Management </heading><chunk>First, the system should know whether a user ut- terance is a question, an answer, or has some other function (Algorithm 1). If the user utterance is a question, KB-QA will answer. If the utterance is an answer, the system will check whether or not the user utterance is correct. Otherwise, we used the example based dialog system which uses a similarity measure to find an example sentence in the example DB (Nio et al., 2014), and utters the sentence (Table 1). The following are the system actions such as Answer, Question (entity, prop- erty), Inform (entity, property, obj, Check- UserAnswer. To generate the next system utter- ance, we should select arguments such as entity, property, and object. For example, Question (entity=Bill Gates, property=or- ganization.founded) will generate Do you Algorithm 1 : RuleBasedDA (U,i i i i1 ) Require: i i: user utterance Require: i i i i1 : previous system action 1: i i2i i2 U contains WH questions and IsEntity(U) 2: i i2i i2i i2i i2 i ii i  = i i: i ii ii ii i i ii ii ii i 3: i i2i i2i i2i i2 if i i i i1 is S:Question 4: i i2i i2i i2i i2 i ii i  = i i: i i i ii i i ii ii i 5: i i2i i2i i2i i2 6: i i2i i2i i2i i2 i ii i  = i i: i ii ihi ii ii i  Algorithm 1: Generation Algorithm, IsEntity returns true is when entity is detected in user utterance Figure 2: Procedure of property embedding Table 1: Example dialog and user dialog act and system action (S:system, U:user) Utterance Dialog Act U:Hi, nice to meet you. U:others S:Hello, good to see you. Matched Exam- ple U:Who is Bill Gates? U:question S:Bill Gates is organization learner and programmer. S:Answer S:Do you know what company Bill Gates founded? S:Question U:Microsoft U:answer S: Thats right. S: Bill Gates founded Microsoft with Paul Allen S:CheckAnswer S: Inform Figure 1: current knowledge graph is undirected graph and proposed knowledge graph is directed weighted graph. (* denote the weight is 0 which meant the tutor never asked about this question) 140 want to know the company Bill Gates founded? Inform(entity=Bill Gates, property= or- ganization.founded,obj=Microsoft) will generate Bill Gates founded Microsoft In this paper, we mainly explore how to select the most appropriate entity and property for generat- ing system utterances. </chunk></section><section><heading>3.2 Weight between two entities </heading><chunk>Freebase is stored in a graph structure. The entity Bill Gates is linked to many properties and enti- ties in triple format. However, the edges are not weighted. When the system provides useful infor- mation to a user about Bill Gates, then his profes- sion, or books that he wrote will be more interest- ing to a user than Gates gender or URL infor- mation. Moreover, the relationship between two entities can be represented as a directional graph. When we explain about Bill Gates, Basic pro- graming language is important because he used it when he was programming. However, when we explain about Basic programing language, Bill Gates is not very important. Entities in Wikipedia are linked (Mendes et al., 2011) to obtain the weight information. Weight w(i i i i , i i i i ) is obtained as the follows when i i i i is Bill Gates and i i i i is Microsoft; First, we need the number of occur- rence of Microsoft entity in the Bill Gates Wikipedia page to get i i1i ii ii i(i i i i ) i i i i . Second, we search the shortest path from Bill Gates to Mi- crosoft in Freebase KB graph, then count the number of properties to get n(i i i i , i i i i ). w(i i i i , i i i i ) = i i14 i i1i ii ii i(i i i i ) i i i i i i1i ii ii i(i i i i ) i i i i i i i i i i i i + 1 n(i i i i ,i i i i ) (1) i i1i ii ii i(i i i i ) i i i i denotes frequency of i i i i in Wikipedia i i i i page. i i i i denotes all entities in the Wikipedia i i i i page. n(i i i i , i i i i ) denotes # of hops between i i i i and i i i i (e.g., n(Billl Gates, Microsoft) = 1, n(Bill Gates, Microsoft Windows) = 2 in Figure 1-(a)) We eliminate edges that have w(i i i i , i i i i ) = 0 and nodes where n(i i i i , i i i i ) &gt; 2 (a more than 3 hop re- lationship). i i14 and are currently set to 1. </chunk></section><section><heading>3.3 Property Embedding </heading><chunk>The intuition of property-embedding similarity is as follows: when a user is talking about Bill Gates professional achievement, POMY-ITSs best op- tion would be to explain something related to pro- fessional achievement. However, designing all possible replies manually would be too expensive. When a user asks about Bill Gates parents, POMY-ITSs best option would be to explain or ask the user about Gates other family members. To determine that the people.person.parents property is more similar to people.person.chil- dren than people.person.employment_history (Figure 5), property-embedding vectors are gen- erated to compute the similarity between two properties. We first obtain the sequence of the property from the Wikipedia corpus (Figure 2), then we use Skip-gram to train the vectors (Figure 3). The training objective of the Skip-gram model is to find word representations that are useful to predict the surrounding (Mikolov et al., 2013). We used skip-gram to predict the next property i i given the current property as the following equa- tion: 1 i i i ii ii ii i(i i i i+i i 2&lt;i i&lt;2,i i=0 i i i i=1 |i i i i ) (2) i ihi ii ii i i i i i denotes current property. The basic Skip-gram formulation uses the soft-max function to define i i(i i i i+i i |i i i i ): p(i i i i |i i i i14 ) = exp(i i i i i i i i i i i i14 ) i ii ii i(i i i i i i i i i i14 ) i i i i=1 (3) where i i i i and i i i i are, respectively, the input and output vector representations of r, and R is the number of properties in Freebase. </chunk></section><section><heading>3.4 System Utterance Generation </heading><chunk>After choosing entity and property, we can gener- ate either question or inform sentences. Template- based natural language generation uses rules (Ta- ble 2) to generate question utterances. Questions begin with a question word, are followed by the Figure 3 Skip-gram of property embedding Figure 2: Procedure of property embedding 141 Freebase description of the expected answer type d(t), the further followed by Freebase descriptions of entities d(e) and d(p). To fill in auxiliary verbs, determiners, and prepositions, we parse the de- scription d(p) into one of NP, VP, PP, or NP VP. For inform system actions, we generate the sen- tences from triple &lt;Bill Gates, organiza- tion.founded, Microsoft&gt; to Bill Gates founded Microsoft as follows: extract the triple from the text, and disambiguate to KB entities. Then, align to existing triples in KB, fourth. Finally, collect matched phrase-property pairs from aligned tri- ples. Table 2: Template of questioning. WH represents Do you know what. Rule Example WH d(t) has d(e) as NP? WH election contest has George Bush as winner? WH d(t) (AUX) VP d(e)? WH radio station serves area New- York? WH PP d(e) ? WH beer from region Argentina? WH d(t) VP the NP d(e)? WH mass transportation system served the area Berlin? </chunk></section><section><heading>3.5 Experiment and Result </heading><chunk>To compare the weight of two entities, 10 human experts ranked among the 60 entities that were most closely related to the target entity. We asked them to rank the entities as if they were teaching students about the target entities such as Bill Gates, Steve Jobs, Seoul, etc. We considered the human labeled rankings to be the correct an- swers, and compared them to answers provided by the proposed method and word2vec 2 (Figure 4); as a similarity statistic we used the average score of Mean reciprocal rank (MRR). We obtained MRR scores 10 times, then got mean and standard deviation by repeating one human labels as the an- swer and another human labels as the test; this al- lows quantification of the correlation between hu- man labels. The results show that human-to-hu- man has the highest correlation. Next, the correla- tion between human and the proposed method is significantly better than between human and word2vec (Figure 4). We found that word2vec has high similarity when entities are of the same type; e.g., Melinda Gates, Steve Ballmer, and Jeff are all person in Table 3. However, humans and the proposed system selected entities of different types such as Microsoft and Windows. Thus, semantic similarity does not necessarily represent the most related entities for explanation about the target entity in the educational perspective. To show property similarity, we plot in the 2D space using t-SNE (Van der Maaten and Hinton, 2008). 2 The model of freebase entity embedding is already availa- ble in https://code.google.com/p/word2vec/ The graph shows that similar properties are closely plotted in 2D space, especially people.per- son.children and people.person.parents (Figure 5). This is exactly consistent with our purpose of property-embedding, and our property-embed- ding model is available 3 which includes 779 total properties and 100 dimension. </chunk></section><section><heading>4 Conclusion </heading><chunk>We developed a conversational knowledge-teach- ing agent using knowledge base for educational purposes. To generate proper system utterance, we obtained the weight between two entities and property similarity. The proposed method signifi- cantly improved upon baseline methods. In the fu- ture, we will improve our conversational agent for knowledge education more tightly integrated into QA systems and dialog systems. 3 http://isoft.postech.ac.kr/~kyusonglee/sigdial/p.emb.vec Table 3: Ranked Results of the top 5 entities gen- erated for Bill Gates Rank Human Proposed Word2Vec 1 Microsoft Microsoft Melinda Gates 2 MS Windows Paul Allen Steve Ballmer 3 MS-DOS Harvard Unv. Bill Melinda Gates Foundation 4 Harvard Univ. Lakeside School Feff_Raikes 5 OS/2 CEO Ray Ozzie B i l l G a t e s S t e v e J o b s S e o u l 0 1 2 3 4 M R R H u m a n P ro p o s e d W o r d 2 v e c Figure 4: Mean and SD of MRR scores for 10 human labeled rankings Figure 5: plotting property-embedding vectors 142 </chunk></section><section><heading>Acknowledgements </heading><chunk>This research was supported by the Basic Science Re- search Program through the National Research Foun- dation of Korea (NRF) funded by the Ministry of Edu- cation, Science and Technology (2010-0019523) and was supported by ATC(Advanced Technology Center) Program-Development of Conversational Q&amp;A Search Framework Based On Linked Data: Project No. 10048448 and was partly supported by Institute for In- formation &amp; communications Technology Promo- tion(IITP) grant funded by the Korea govern- ment(MSIP) (No. R0101-15-0176, Development of Core Technology for Human-like Self-taught Learning based on a Symbolic Approach) </chunk></section><section><heading>References </heading><chunk>Jonathan Berant and Percy Liang. 2014. Semantic pars- ing via paraphrasing. In Proceedings of ACL,vol- ume 7, page 92. Jenny Brusk, Preben Wik, and Anna Hjalmarsson. 2007. Deal a serious game for call practicing con- versational skills in the trade domain. Proceedings of SLATE 2007. Davide Fossati, Barbara Di Eugenio, Christopher Brown, and Stellan Ohlsson. 2008. Learning linked lists: Experiments with the ilist system. In Intelli- gent tutoring systems, pages 8089. Springer. Arthur C Graesser, Shulan Lu, George Tanner Jackson, Heather Hite Mitchell, Mathew Ventura, Andrew Olney, and Max M Louwerse. 2004. Autotutor: A tutor with dialogue in natural language. Behavior Research Methods, Instruments, &amp; Computers, 36(2):180192. WLewis Johnson, NingWang, and ShuminWu. 2007. Experience with serious games for learning foreign languages and cultures. In Proceedings of the Sim- TecT Conference. Pamela Jordan, Patricia Albacete, Michael J Ford, San- dra Katz, Michael Lipschultz, Diane Litman, Scott Silliman, and Christine Wilson. 2013. Interactive event: The rimac tutor-a simulation of the highly in- teractive nature of human tutorial dialogue. In Arti- ficial Intelligence in Education, pages 928929. Springer. LEE Kyusong, Soo-Ok Kweon, LEE Sungjin, NOH Hyungjong, and Gary Geunbae Lee. 2014. Postech immersive english study (pomy): Dialog-based lan- guage learning game. IEICE TRANSACTIONS on Information and Systems, 97(7):18301841. H Chad Lane and Kurt VanLehn. 2005. Teaching the tacit knowledge of programming to noviceswith nat- ural language tutoring. Computer Science Education, 15(3):183201. Sungjin Lee, Hyungjong Noh, Jonghoon Lee, Kyusong Lee, and G Lee. 2010. Postech approaches for dia- log-based english conversation tutoring. Proc. APSIPA ASC, pages 794803. Sungjin Lee, Hyungjong Noh, Jonghoon Lee, Kyusong Lee, Gary Geunbae Lee, Seongdae Sagong, and Munsang Kim. 2011. On the effectiveness of ro- botassisted language learning. ReCALL, 23(01):25 58. Diane J Litman and Scott Silliman. 2004. Itspoke: An intelligent tutoring spoken dialogue system. In Demonstration papers at HLT-NAACL 2004, pages 58. Association for Computational Linguistics. Pablo N Mendes, Max Jakob, Andr es Garc a-Silva, and Christian Bizer. 2011. Dbpedia spotlight: shed- ding light on the web of documents. In Proceedings of the 7th International Conference on Semantic Systems, pages 18. ACM. Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient estimation of word represen- tations in vector space. arXiv preprint arXiv:1301.3781. Hazel Morton and Mervyn A Jack. 2005. Scenari- obased spoken interaction with virtual agents. Com- puter Assisted Language Learning, 18(3):171191. Lasguido Nio, Sakriani Sakti, Graham Neubig, Tomoki Toda, and Satoshi Nakamura. 2014. Improving the robustness of example-based dialog retrieval using recursive neural network paraphrase identification. SLT 2014. Laurens Van der Maaten and Geoffrey Hinton. 2008. Visualizing data using t-sne. Journal of Machine Learning Research, 9(2579-2605):85. Kurt VanLehn, PamelaWJordan, Carolyn P Ros e, Dumisizwe Bhembe, Michael B ottner, Andy Gaydos, Maxim Makatchev, Umarani Pappuswamy, Michael Ringenberg, Antonio Roque, et al. 2002. The architec- ture of why2-atlas: A coach for qualitative physics essay writing. In Intelligent tutoring systems, pages 158167. Springer. Kurt Vanlehn, Collin Lynch, Kay Schulze, Joel A Shapiro, Robert Shelby, Linwood Taylor, Don Treacy, Anders Weinstein, and Mary Wintersgill. 2005. The andes physics tutoring system: Lessons learned. International Journal of Artificial Intelli- gence in Education, 15(3):147204. 143 </chunk></section></sec_map>